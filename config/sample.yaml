model_name_or_path: "rinna/llama-3-youko-8b"
output_dir: "./results/rinna/llama-3-youko-8b-sharegpt-sft-annotation-all-240612-sft"
adapter_path: "./results/rinna/llama-3-youko-8b-sharegpt-sft/checkpoint-2000"
sampling_probabilities: null
dataset_format: "alpaca"
datasets: 
  - "studio-ousia/annotation-240224"
  - "studio-ousia/annotation-240326"
  - "studio-ousia/annotation-Summarize_240608"
  - "studio-ousia/annotation-OpenQA_240608"
  - "studio-ousia/annotation-Brainstorm_240608"
bits: 8
per_device_train_batch_size: 1
per_device_eval_batch_size: 8 
gradient_accumulation_steps: 4
learning_rate: 0.00002
max_steps: 200
save_steps: 100
save_total_limit: 2
logging_steps: 50
save_strategy: "steps"
data_seed: 42
max_new_tokens: 32
dataloader_num_workers: 1
group_by_length: true
logging_strategy: "steps"
remove_unused_columns: false
do_train: true
lora_r: 128
lora_alpha: 256
lora_dropout: 0.05
double_quant: true
quant_type: "nf4"
warmup_steps: 100
lr_scheduler_type: "cosine"
gradient_checkpointing: true
source_max_len: 16
target_max_len: 512
adam_beta2: 0.999
max_grad_norm: 0.3
weight_decay: 0.01
neftune_noise_alpha: 5
seed: 42
trust_remote_code: true
report_to: "wandb"
do_eval: false
